{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Model Evaluation Notebook\n",
    "\n",
    "Comprehensive evaluation of the bug prediction model.\n",
    "\n",
    "**Contents:**\n",
    "1. Load trained model and test data\n",
    "2. Calculate metrics and confusion matrix\n",
    "3. Error analysis (false positives/negatives)\n",
    "4. Feature importance analysis\n",
    "5. Ablation study (features-only vs tokens-only vs hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "sys.path.append('..')  # Add project root to path\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "from src.model import create_model, create_data_loaders\n",
    "from src.utils import get_device, load_processed_data, load_metadata\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "project_root = Path('..')\n",
    "models_dir = project_root / 'models'\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_metadata(str(processed_dir))\n",
    "print('Dataset Metadata:')\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "data = load_processed_data(str(processed_dir))\n",
    "\n",
    "features = data['features']\n",
    "labels = data['labels']\n",
    "token_sequences = data['token_sequences']\n",
    "train_idx = data['train_idx']\n",
    "val_idx = data['val_idx']\n",
    "test_idx = data['test_idx']\n",
    "\n",
    "print(f'Features shape: {features.shape}')\n",
    "print(f'Token sequences shape: {token_sequences.shape}')\n",
    "print(f'Test set size: {len(test_idx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature names\n",
    "with open(processed_dir / 'feature_names.json') as f:\n",
    "    feature_names = json.load(f)\n",
    "print(f'Feature names ({len(feature_names)}): {feature_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config to get model settings\n",
    "import yaml\n",
    "with open(project_root / 'config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model_type = config['model'].get('model_type', 'hybrid')\n",
    "use_lstm = config['model'].get('use_lstm', True)\n",
    "embedding_dim = config['model'].get('embedding_dim', 128)\n",
    "\n",
    "print(f'Model type: {model_type}')\n",
    "print(f'Use LSTM: {use_lstm}')\n",
    "print(f'Embedding dim: {embedding_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "device = get_device(False)  # Use CPU\n",
    "model_path = models_dir / 'best_model.pth'\n",
    "\n",
    "model = create_model(\n",
    "    model_type=model_type,\n",
    "    num_features=metadata['n_features'],\n",
    "    vocab_size=metadata.get('vocab_size', 5000),\n",
    "    use_lstm=use_lstm,\n",
    "    embedding_dim=embedding_dim\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model loaded from {model_path}')\n",
    "print(f'Total parameters: {total_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 2. Model Predictions and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data\n",
    "test_features = torch.FloatTensor(features[test_idx]).to(device)\n",
    "test_tokens = torch.LongTensor(token_sequences[test_idx]).to(device)\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    if model_type == 'hybrid':\n",
    "        y_prob = model(test_features, test_tokens).cpu().numpy().flatten()\n",
    "    else:\n",
    "        y_prob = model(test_features).cpu().numpy().flatten()\n",
    "\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "y_true = test_labels\n",
    "\n",
    "print(f'Predictions generated for {len(y_true)} test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(y_true, y_pred),\n",
    "    'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "    'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "    'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "    'roc_auc': roc_auc_score(y_true, y_prob)\n",
    "}\n",
    "\n",
    "print('=' * 50)\n",
    "print('TEST SET METRICS')\n",
    "print('=' * 50)\n",
    "for name, value in metrics.items():\n",
    "    print(f'  {name:12s}: {value:.4f} ({value*100:.2f}%)')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_true, y_pred, target_names=['Clean', 'Buggy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute counts\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=['Clean', 'Buggy'], yticklabels=['Clean', 'Buggy'])\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "ax1.set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Percentages\n",
    "ax2 = axes[1]\n",
    "cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Blues', ax=ax2,\n",
    "            xticklabels=['Clean', 'Buggy'], yticklabels=['Clean', 'Buggy'])\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "ax2.set_title('Confusion Matrix (Percentages)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'logs' / 'confusion_matrix_detailed.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nTrue Negatives (correctly clean): {cm[0,0]}')\n",
    "print(f'False Positives (clean flagged buggy): {cm[0,1]}')\n",
    "print(f'False Negatives (buggy missed): {cm[1,0]}')\n",
    "print(f'True Positives (correctly buggy): {cm[1,1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision-Recall curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "ax1 = axes[0]\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "ax1.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {metrics[\"roc_auc\"]:.4f})')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax1.fill_between(fpr, tpr, alpha=0.2)\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax2 = axes[1]\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "ax2.plot(recall, precision, 'g-', linewidth=2, label=f'PR (F1 = {metrics[\"f1\"]:.4f})')\n",
    "ax2.axhline(y=y_true.mean(), color='k', linestyle='--', label='Baseline')\n",
    "ax2.fill_between(recall, precision, alpha=0.2, color='green')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curve')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'logs' / 'roc_pr_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 3. Error Analysis\n",
    "\n",
    "### What are False Positives and False Negatives?\n",
    "\n",
    "- **False Positive (FP)**: Clean code incorrectly flagged as buggy\n",
    "  - Impact: Developer wastes time investigating non-issues\n",
    "  - Too many FPs lead to developers ignoring the tool\n",
    "\n",
    "- **False Negative (FN)**: Buggy code incorrectly marked as clean  \n",
    "  - Impact: Real bugs slip through to production\n",
    "  - More dangerous but less annoying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original code for error analysis\n",
    "raw_df = pd.read_csv(project_root / 'data' / 'raw' / 'functions.csv')\n",
    "print(f'Loaded {len(raw_df)} functions from raw data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find false positives and false negatives\n",
    "test_indices = test_idx  # Global indices\n",
    "\n",
    "# Get indices relative to test set\n",
    "fp_mask = (y_true == 0) & (y_pred == 1)  # Predicted buggy, actually clean\n",
    "fn_mask = (y_true == 1) & (y_pred == 0)  # Predicted clean, actually buggy\n",
    "tp_mask = (y_true == 1) & (y_pred == 1)  # Correctly predicted buggy\n",
    "tn_mask = (y_true == 0) & (y_pred == 0)  # Correctly predicted clean\n",
    "\n",
    "fp_indices = test_indices[fp_mask]\n",
    "fn_indices = test_indices[fn_mask]\n",
    "\n",
    "print(f'False Positives: {len(fp_indices)} (clean code flagged as buggy)')\n",
    "print(f'False Negatives: {len(fn_indices)} (buggy code missed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_error_case(idx, prob, case_type, num):\n",
    "    \"\"\"Display an error case with code and features.\"\"\"\n",
    "    row = raw_df.iloc[idx]\n",
    "    \n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    print(f'{case_type} #{num}')\n",
    "    print(f'Probability: {prob:.4f} (Threshold: 0.5)')\n",
    "    print(f'Function: {row[\"function_name\"]} from {row[\"repo\"]}')\n",
    "    print(f'{\"=\"*70}')\n",
    "    print(row['code'])\n",
    "    print('=' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 10 False Positives (clean code flagged as buggy)\n",
    "print('\\n' + '='*70)\n",
    "print('FALSE POSITIVES: Clean code incorrectly flagged as BUGGY')\n",
    "print('These waste developer time investigating non-issues')\n",
    "print('='*70)\n",
    "\n",
    "# Sort by confidence (highest probability first - most confident mistakes)\n",
    "fp_probs = y_prob[fp_mask]\n",
    "fp_sorted_idx = np.argsort(fp_probs)[::-1]  # Descending\n",
    "\n",
    "for i, idx in enumerate(fp_sorted_idx[:10]):\n",
    "    global_idx = fp_indices[idx]\n",
    "    prob = fp_probs[idx]\n",
    "    display_error_case(global_idx, prob, 'FALSE POSITIVE', i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 10 False Negatives (buggy code missed)\n",
    "print('\\n' + '='*70)\n",
    "print('FALSE NEGATIVES: Buggy code incorrectly marked as CLEAN')\n",
    "print('These are dangerous - real bugs slip through to production')\n",
    "print('='*70)\n",
    "\n",
    "# Sort by confidence (lowest probability first - most confident wrong predictions)\n",
    "fn_probs = y_prob[fn_mask]\n",
    "fn_sorted_idx = np.argsort(fn_probs)  # Ascending (most confidently wrong)\n",
    "\n",
    "for i, idx in enumerate(fn_sorted_idx[:10]):\n",
    "    global_idx = fn_indices[idx]\n",
    "    prob = fn_probs[idx]\n",
    "    display_error_case(global_idx, prob, 'FALSE NEGATIVE', i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze probability distribution for errors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.hist(y_prob[y_true == 0], bins=30, alpha=0.6, label='Actually Clean', color='green')\n",
    "ax1.hist(y_prob[y_true == 1], bins=30, alpha=0.6, label='Actually Buggy', color='red')\n",
    "ax1.axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "ax1.set_xlabel('Predicted Bug Probability')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Probability Distribution by True Label')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[1]\n",
    "categories = ['True Negatives', 'False Positives', 'False Negatives', 'True Positives']\n",
    "counts = [tn_mask.sum(), fp_mask.sum(), fn_mask.sum(), tp_mask.sum()]\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c', '#3498db']\n",
    "bars = ax2.bar(categories, counts, color=colors)\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Prediction Categories')\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "             str(count), ha='center', fontsize=10)\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'logs' / 'error_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis\n",
    "\n",
    "Understanding which features contribute most to bug detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of features with bug labels\n",
    "correlations = []\n",
    "for i, name in enumerate(feature_names):\n",
    "    corr = np.corrcoef(features[:, i], labels)[0, 1]\n",
    "    correlations.append((name, corr))\n",
    "\n",
    "# Sort by absolute correlation\n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print('Feature Correlations with Bug Labels (sorted by |correlation|):')\n",
    "print('-' * 50)\n",
    "for name, corr in correlations:\n",
    "    direction = '+' if corr > 0 else '-'\n",
    "    print(f'  {name:30s}: {corr:+.4f} {direction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 15 feature correlations\n",
    "top_n = 15\n",
    "names = [c[0] for c in correlations[:top_n]]\n",
    "corrs = [c[1] for c in correlations[:top_n]]\n",
    "colors = ['#e74c3c' if c > 0 else '#2ecc71' for c in corrs]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(top_n), corrs, color=colors)\n",
    "plt.yticks(range(top_n), names)\n",
    "plt.xlabel('Correlation with Bug Label')\n",
    "plt.title('Top Feature Correlations with Bugs\\n(Red = higher value → more bugs, Green = higher value → fewer bugs)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'logs' / 'feature_correlations.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importance\n",
    "# Shuffle each feature and measure accuracy drop\n",
    "\n",
    "def compute_accuracy(model, features, tokens, labels, device, model_type):\n",
    "    \"\"\"Compute accuracy for given features.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        feat_tensor = torch.FloatTensor(features).to(device)\n",
    "        tok_tensor = torch.LongTensor(tokens).to(device)\n",
    "        if model_type == 'hybrid':\n",
    "            probs = model(feat_tensor, tok_tensor).cpu().numpy().flatten()\n",
    "        else:\n",
    "            probs = model(feat_tensor).cpu().numpy().flatten()\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "# Baseline accuracy\n",
    "baseline_acc = compute_accuracy(model, features[test_idx], token_sequences[test_idx], \n",
    "                                 labels[test_idx], device, model_type)\n",
    "print(f'Baseline accuracy: {baseline_acc:.4f}')\n",
    "\n",
    "# Compute permutation importance\n",
    "importances = []\n",
    "n_repeats = 5\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    scores = []\n",
    "    for _ in range(n_repeats):\n",
    "        # Create copy and shuffle feature\n",
    "        features_perm = features[test_idx].copy()\n",
    "        np.random.shuffle(features_perm[:, i])\n",
    "        \n",
    "        # Compute accuracy with shuffled feature\n",
    "        perm_acc = compute_accuracy(model, features_perm, token_sequences[test_idx],\n",
    "                                     labels[test_idx], device, model_type)\n",
    "        scores.append(baseline_acc - perm_acc)  # Accuracy drop\n",
    "    \n",
    "    importances.append((name, np.mean(scores), np.std(scores)))\n",
    "\n",
    "# Sort by importance\n",
    "importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('\\nPermutation Importance (accuracy drop when feature is shuffled):')\n",
    "print('-' * 60)\n",
    "for name, imp, std in importances[:15]:\n",
    "    print(f'  {name:30s}: {imp:+.4f} (+/- {std:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation importance\n",
    "top_n = 15\n",
    "names = [imp[0] for imp in importances[:top_n]]\n",
    "values = [imp[1] for imp in importances[:top_n]]\n",
    "errors = [imp[2] for imp in importances[:top_n]]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(top_n), values, xerr=errors, color='steelblue', capsize=3)\n",
    "plt.yticks(range(top_n), names)\n",
    "plt.xlabel('Accuracy Drop When Feature Shuffled')\n",
    "plt.title('Permutation Feature Importance\\n(Higher = more important for predictions)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'logs' / 'permutation_importance.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions for buggy vs clean\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get top 9 features by importance\n",
    "top_features = [imp[0] for imp in importances[:9]]\n",
    "\n",
    "for i, (ax, name) in enumerate(zip(axes, top_features)):\n",
    "    feat_idx = feature_names.index(name)\n",
    "    \n",
    "    clean_vals = features[labels == 0, feat_idx]\n",
    "    buggy_vals = features[labels == 1, feat_idx]\n",
    "    \n",
    "    ax.hist(clean_vals, bins=30, alpha=0.6, label='Clean', color='green')\n",
    "    ax.hist(buggy_vals, bins=30, alpha=0.6, label='Buggy', color='red')\n",
    "    ax.set_title(name)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Feature Distributions: Buggy vs Clean Code', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'logs' / 'feature_distributions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 5. Ablation Study\n",
    "\n",
    "### Why Test Ablations?\n",
    "\n",
    "An ablation study removes components to understand their contribution:\n",
    "\n",
    "1. **Features-only**: Only numerical features, no token sequences\n",
    "2. **Tokens-only**: Only token sequences, zero numerical features  \n",
    "3. **Full Hybrid**: Both features and tokens combined\n",
    "\n",
    "This helps us understand:\n",
    "- Which input stream provides most value\n",
    "- Whether complexity (tokens + LSTM) is worth the cost\n",
    "- Optimal architecture for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Current model (Full Hybrid or whatever is configured)\n",
    "print('='*60)\n",
    "print('ABLATION STUDY')\n",
    "print('='*60)\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "# Full model (already computed)\n",
    "ablation_results.append({\n",
    "    'model': f'Full Model ({model_type})',\n",
    "    'accuracy': metrics['accuracy'],\n",
    "    'f1': metrics['f1'],\n",
    "    'roc_auc': metrics['roc_auc']\n",
    "})\n",
    "print(f'\\n1. Full Model ({model_type}):')\n",
    "print(f'   Accuracy: {metrics[\"accuracy\"]:.4f}')\n",
    "print(f'   F1 Score: {metrics[\"f1\"]:.4f}')\n",
    "print(f'   ROC-AUC: {metrics[\"roc_auc\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Features only (zero out tokens by using random/zeroed input)\n",
    "# We'll use the same model but with zeroed token sequences\n",
    "if model_type == 'hybrid':\n",
    "    print('\\n2. Features Only (zeroed token sequences):')\n",
    "    \n",
    "    # Create zeroed tokens (all padding)\n",
    "    zero_tokens = torch.zeros_like(test_tokens)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_prob_feat = model(test_features, zero_tokens).cpu().numpy().flatten()\n",
    "    \n",
    "    y_pred_feat = (y_prob_feat >= 0.5).astype(int)\n",
    "    \n",
    "    feat_metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred_feat),\n",
    "        'f1': f1_score(y_true, y_pred_feat, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob_feat)\n",
    "    }\n",
    "    \n",
    "    ablation_results.append({\n",
    "        'model': 'Features Only',\n",
    "        **feat_metrics\n",
    "    })\n",
    "    \n",
    "    print(f'   Accuracy: {feat_metrics[\"accuracy\"]:.4f}')\n",
    "    print(f'   F1 Score: {feat_metrics[\"f1\"]:.4f}')\n",
    "    print(f'   ROC-AUC: {feat_metrics[\"roc_auc\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Tokens only (zero out numerical features)\n",
    "if model_type == 'hybrid':\n",
    "    print('\\n3. Tokens Only (zeroed numerical features):')\n",
    "    \n",
    "    # Create zeroed features\n",
    "    zero_features = torch.zeros_like(test_features)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_prob_tok = model(zero_features, test_tokens).cpu().numpy().flatten()\n",
    "    \n",
    "    y_pred_tok = (y_prob_tok >= 0.5).astype(int)\n",
    "    \n",
    "    tok_metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred_tok),\n",
    "        'f1': f1_score(y_true, y_pred_tok, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob_tok)\n",
    "    }\n",
    "    \n",
    "    ablation_results.append({\n",
    "        'model': 'Tokens Only',\n",
    "        **tok_metrics\n",
    "    })\n",
    "    \n",
    "    print(f'   Accuracy: {tok_metrics[\"accuracy\"]:.4f}')\n",
    "    print(f'   F1 Score: {tok_metrics[\"f1\"]:.4f}')\n",
    "    print(f'   ROC-AUC: {tok_metrics[\"roc_auc\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ablation results\n",
    "if len(ablation_results) > 1:\n",
    "    ablation_df = pd.DataFrame(ablation_results)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    metrics_to_plot = ['accuracy', 'f1', 'roc_auc']\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "    \n",
    "    for ax, metric, color in zip(axes, metrics_to_plot, colors):\n",
    "        bars = ax.bar(ablation_df['model'], ablation_df[metric], color=color)\n",
    "        ax.set_ylabel(metric.upper())\n",
    "        ax.set_title(f'{metric.upper()} by Model Configuration')\n",
    "        ax.set_ylim(0, 1)\n",
    "        for bar, val in zip(bars, ablation_df[metric]):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                   f'{val:.3f}', ha='center', fontsize=10)\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(project_root / 'logs' / 'ablation_study.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nAblation Study Summary:')\n",
    "    print(ablation_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation\n",
    "print('\\n' + '='*60)\n",
    "print('ABLATION STUDY INTERPRETATION')\n",
    "print('='*60)\n",
    "\n",
    "if len(ablation_results) > 1:\n",
    "    full_acc = ablation_results[0]['accuracy']\n",
    "    feat_acc = ablation_results[1]['accuracy']\n",
    "    tok_acc = ablation_results[2]['accuracy'] if len(ablation_results) > 2 else 0\n",
    "    \n",
    "    print(f'\\n1. Numerical Features Contribution:')\n",
    "    print(f'   Features alone: {feat_acc:.4f} accuracy')\n",
    "    print(f'   Contribution: {(full_acc - tok_acc)*100:.1f}% of model performance')\n",
    "    \n",
    "    print(f'\\n2. Token Sequences Contribution:')\n",
    "    print(f'   Tokens alone: {tok_acc:.4f} accuracy')\n",
    "    print(f'   Contribution: {(full_acc - feat_acc)*100:.1f}% of model performance')\n",
    "    \n",
    "    print(f'\\n3. Synergy (combined > sum of parts):')\n",
    "    synergy = full_acc - (feat_acc + tok_acc - 0.5)  # 0.5 = random baseline\n",
    "    print(f'   Combined effect: {synergy*100:+.1f}%')\n",
    "    \n",
    "    if feat_acc > tok_acc:\n",
    "        print('\\n=> RECOMMENDATION: Numerical features are more important.')\n",
    "        print('   Consider using FeatureOnlyModel for faster inference.')\n",
    "    else:\n",
    "        print('\\n=> RECOMMENDATION: Token sequences are more important.')\n",
    "        print('   The LSTM/embedding approach adds value.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 6. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('MODEL EVALUATION SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "print(f'\\nModel: {model_type} with {total_params:,} parameters')\n",
    "print(f'Test Set: {len(y_true)} samples ({y_true.sum()} buggy, {len(y_true) - y_true.sum()} clean)')\n",
    "\n",
    "print(f'\\nPerformance Metrics:')\n",
    "print(f'  - Accuracy:  {metrics[\"accuracy\"]*100:.2f}%')\n",
    "print(f'  - Precision: {metrics[\"precision\"]*100:.2f}% (of predicted bugs, this many are real)')\n",
    "print(f'  - Recall:    {metrics[\"recall\"]*100:.2f}% (of actual bugs, we catch this many)')\n",
    "print(f'  - F1 Score:  {metrics[\"f1\"]*100:.2f}% (harmonic mean of precision/recall)')\n",
    "print(f'  - ROC-AUC:   {metrics[\"roc_auc\"]*100:.2f}% (ability to rank bugs higher than clean)')\n",
    "\n",
    "print(f'\\nError Analysis:')\n",
    "print(f'  - False Positives: {fp_mask.sum()} (clean code flagged as buggy)')\n",
    "print(f'  - False Negatives: {fn_mask.sum()} (buggy code missed)')\n",
    "\n",
    "print(f'\\nTop 5 Most Important Features:')\n",
    "for name, imp, _ in importances[:5]:\n",
    "    print(f'  - {name}: {imp:+.4f} importance')\n",
    "\n",
    "print('\\n' + '='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to JSON\n",
    "summary = {\n",
    "    'model_type': model_type,\n",
    "    'total_parameters': total_params,\n",
    "    'test_set_size': len(y_true),\n",
    "    'metrics': metrics,\n",
    "    'confusion_matrix': {\n",
    "        'true_negatives': int(cm[0, 0]),\n",
    "        'false_positives': int(cm[0, 1]),\n",
    "        'false_negatives': int(cm[1, 0]),\n",
    "        'true_positives': int(cm[1, 1])\n",
    "    },\n",
    "    'top_features': [{'name': n, 'importance': float(i)} for n, i, _ in importances[:10]],\n",
    "    'ablation_results': ablation_results\n",
    "}\n",
    "\n",
    "with open(project_root / 'logs' / 'evaluation_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print('Summary saved to logs/evaluation_summary.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
